Tb6331d8c78e9b572d3919fc18cf4573b	own_claim 332 374	We use the automated calibration technique
T1e3b976abdeeda9d2acb7cef08bddd08	data 380 402	White and Forsyth 2005
Tdc805218f19f56ebe340d564d26085d4	own_claim 409 443	any standard calibration will work
T634141f73145e772d56b2336ddf3af7d	data 446 456	Zhang 2002
Tc08fe3c6984d7d2a9d0180e21841d3de	data 463 475	Bouguet 2005
T7f710a7c36e8f2aabee78c6b775b07f1	background_claim 478 494	are good choices
T0e49f8bf468a1b8d1011628ddd746bd4	own_claim 588 617	Adequate lighting is critical
T8eb94a075c99b030827b743fc6cd1d4f	data 624 638	our experience
T5e5177bbb7758f851636acd72becad7a	own_claim 639 671	fewer lights degrade performance
T4350517a67308019580fda2def14ddf6	data 679 692	harsh shadows
T84ceec55736605bab88b47f585fe0710	own_claim 697 728	dim lighting causes motion blur
T79123c612fd3a65921882fd2d6c8f9b0	data 737 758	slower shutter speeds
T9c3f347d6bc0b4cd594ed68472760243	data 834 854	a P4 2.4 GHz machine
T77eab95c6e2dd5e22f0c224e3305230c	own_claim 856 931	acquisition takes roughly 6 minutes and mesh processing 2 minutes per frame
T75cce08e3e34194c5162638423a558da	own_claim 1084 1166	Our capture results are best evaluated by looking at our video and figures 1,12,13
T9f01ceb40a0810bcea7a12f2d081f687	data 1177 1220	to compare against other capture techniques
T07bd6ca3f46cacc7a045d24b3d7b200c	own_claim 1222 1309	it is also necessary to evaluate on several numerical criteria for each capture session
T49f33cc4eaf496a1cff521d5681637ed	own_claim 1599 1627	The sleeve example is unique
Tbb406fd969193c39994bad981262ce8b	data 1636 1677	it was one of the first items we captured
T3f925310f407f390a69cd9f89e42055d	own_claim 1679 1742	Much of the cloth is in contact with the floor and unobservable
T420aa512cddb17596ac23fb1ddc484e4	own_claim 1745 1774	yielding fewer bits of strain
T17a83b57642e68d6e67a8a6cc3a08f2a	own_claim 1789 1846	the camera images were not output in a linear color space
T49d215601eb356625ec038f239a739a7	own_claim 1848 1881	reducing the number of color bits
T8f752d09bf7ed813441c2ae8cbe91d1d	own_claim 1896 1947	we terminated the correspondence algorithm at N = 2
Tf2f779a15bcf3e181fcc0616105d62d1	own_claim 1949 1999	Our pants animation is by far the most challenging
Ta9cd4cab2d177fc8970cb9564bf28263	own_claim 2101 2140	there were 979 3D markers per megapixel
T67df1511fc0d33fc4a1a07d88504f24b	data 2145 2188	we factor out the pixels lost to background
T30cadcd31b94139dfce53dabdf6187d8	own_claim 2190 2286	we get 3500 3D markers per foreground megapixel or 282 foreground pixels per recovered 3D marker
T1eec272bce501e6561250bba37dcd61a	own_claim 2288 2350	Our marker observations average 56 pixels per marker per image
T6f06ff2c66c030e1d0468168d1843c14	own_claim 2352 2397	There are several reasons for the discrepancy
T066e68ebab916f627d17293098e8b170	own_claim 2399 2438	markers must be observed multiple times
Td90c85a6cef4dadc9006369f9b7bdaa9	data 2440 2496	approx 44% of 3D markers are observed in 3 or more views
T33cab6e64e36a40a2662c7064ca53971	own_claim 2499 2546	some markers are observed but not reconstructed
T85b468eb6e9c74b90c8a75d11f8e95ad	data 2555 2587	errors or missing correspondence
Tabe7b0032164d83f91dceb8d1aa6b709	own_claim 2594 2641	many pixels are not considered part of a marker
T14545866cc3524557570c7a16797194c	own_claim 2643 2667;2713 2751	they lie in heavy shadow or occupy the edge between two markers
Taee825ccdf7fda2824d5361381715963	own_claim 2813 2850	We use a small set of captured frames
T20bf67c088fe9c1aff9186db5e24295f	own_claim 2891 2960	in combination with MeshIK to skin skeletal human motion capture data
Tb371d9ba2de77767350a31439333a121	data 2963 2972	figure 11
T6280e5639d8b431d433a158a9948f3cd	own_claim 2976 3031	This approach covers a reasonably large range of motion
Tec2057e91731a5a0d9eaf92a48a0529d	own_claim 3037 3059	ignores cloth dynamics
T75984dac5de69f0f7af6fcd860c3f8c2	own_claim 3061 3153	The largest challenge is that captured cloth meshes contain only points on the cloth surface
Tf734b3fbc144cfda38f60ce196c2dace	own_claim 3158 3188	we do not know joint locations
T27ca34e57da6308ded05daca65fea78e	own_claim 3199 3273	we insert proxy points for knee and hip joints in each of our basis meshes
T2aceb301e42dbd5f88cf7a54ddfb50b5	own_claim 3595 3686	Using our MATLAB implementation of MeshIK, this process takes around 5-10 seconds per frame
T2dc2f60e231e92cab5e51e5aef8ba369	data 3761 3823	for a small basis to adequately express a full range of motion
Tf6a739ca0bf7df375769c2e20ac3f52b	own_claim 3825 3859	each basis pose must be an extreme
T06b9802b12c72e25be03f95adf823477	data 3879 3912	simple objects such as a cylinder
T9e0ce0e9396a85a10ea02acde4e3fa6a	own_claim 3914 3926	a small bend
T27e8540e5030b53f333f6d4510b0edef	own_claim 3941 3986	is sufficient to extrapolate to a larger bend
T288050ef8f10f78150ca0ce00fdb85c5	data 3989 4007	Sumner et al. 2005
T3d657f6bda24eda30b93e8cd9307097e	own_claim 4019 4061	for pants the relationship is more complex
Tfb0af7c171b950de0f72c26b788b09b1	own_claim 4063 4170	the fact that no folding occurs in a small bend does not imply that folding will be absent in a larger bend
T23851be913daea406c81de2f0ae911d8	data 4187 4236	a decent amount of folding occurs in a small bend
Tb799167d09918f8b9d9e11ccedb98338	own_claim 4238 4299	we do not expect extreme folds in a corresponding larger bend
Tf02e880e332571c2be09af2eaa4eda8e	own_claim 4314 4403	MeshIK is most useful when a basis is carefully chosen to prevent extrapolation artifacts
T9263a61a156699833a507d4c4b1c0553	own_claim 4405 4475	One drawback to our approach is the loss of secondary kinematic motion
Tbbe1c9128d5c465f8789a65ebcc5e899	data 4485 4508	the sway of loose cloth
T66fba6ca7bb4b670f207dc17f73c9514	data 4518 4558	MeshIK does not use velocity information
Tc26969116ffdc009a953c192876e5267	own_claim 4560 4598	the resulting animation appears damped

