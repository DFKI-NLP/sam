<H1>6 Results and Applications</H1>
        Our video sequences were taken with synchronized firewire cameras (Foculus FO214C) with a capture resolution of 640 x 480 and a capture rate of 24 frames per second. Our still captures were taken using a digital SLR camera and then downsampled to approximate available video resolutions. We use the automated calibration technique in 
[White and Forsyth 2005], but any standard calibration will work ([Zhang 2002] and [Bouguet 2005]
 are good choices). In the pants sequences, we used seven lights totalling 1550 Watts to illuminate the scene. Adequate lighting is critical: from our experience fewer lights degrade performance due to harsh shadows and dim lighting causes motion blur through slower shutter speeds. Our cloth was printed by a digital mail order fabric printing service. On a P4 2.4 GHz machine, acquisition takes roughly 6 minutes and mesh processing 2 minutes per frame. Code is written in MATLAB.
        
          
        
        observed unobserved backface
        
          <H2>6.1 Capture Results</H2>
          Our capture results are best evaluated by looking at our video and figures 1,12,13. However, to compare against other capture techniques, it is also necessary to evaluate on several numerical criteria for each capture session: cloth pants table drop dance cloth sleeve† # cameras 6 8 18 10 resolution 640x480 640x480 900x600 1500x1000 total markers 853 3060 4793 13465 recovered 819 2405 4361 7557 percentage 96% 79% 91% 56% bits needed 9.7 11.6 12.2 13.7 color bits 6.1 5.1 6.4 4.5 strain bits 9.1 9.4 11.4 ∼ 6.6 †The sleeve example is unique because it was one of the first items we captured. Much of the cloth is in contact with the floor and unobservable – yielding fewer bits of strain. In addition, the camera images were not output in a linear color space, reducing the number of color bits. As a result, we terminated the correspondence algorithm at N = 2. Our pants animation is by far the most challenging, and we analyze some of the details a little more closely. With an average of 2405 observed markers, there were 979 3D markers per megapixel. If we factor out the pixels lost to background, we get 3500 3D markers per foreground megapixel or 282 foreground pixels per recovered 3D marker. Our marker observations average 56 pixels per marker per image. There are several reasons for the discrepancy: markers must be observed multiple times (approx 44% of 3D markers are observed in 3 or more views), some markers are observed but not reconstructed (due to errors or missing correspondence), and many pixels are not considered part of a marker: they lie in heavy shadow
          
            
          
          or occupy the edge between two markers (approx 35% of pixels). 6.2 Retargeting Animations
          We use a small set of captured frames (the previous basis of the 27 examples) in combination with MeshIK to skin skeletal human motion capture data ( figure 11 ). This approach covers a reasonably large range of motion, but ignores cloth dynamics. The largest challenge is that captured cloth meshes contain only points on the cloth surface, so we do not know joint locations. Instead, we insert proxy points for knee and hip joints in each of our basis meshes. These points are then connected to a small set of nearby triangles in the original mesh. For each frame of animation we set the proxy points’ locations according to joint angles in the skeletal mocap data. The resulting transformed joints are used as constraint points in MeshIK, which produces the final output meshes. Using our MATLAB implementation of MeshIK, this process takes around 5-10 seconds per frame. We use the same 27 bases poses for MeshIK based reconstruction. In order for a small basis to adequately express a full range of motion, each basis pose must be an extreme configuration. For simple objects such as a cylinder, a small bend (for example) is sufficient to extrapolate to a larger bend 
[Sumner et al. 2005]. However, for pants the relationship is more complex: the fact that no folding occurs in a small bend does not imply that folding will be absent in a larger bend. Conversely, if a decent amount of folding occurs in a small bend, we do not expect extreme folds in a corresponding larger bend. As a result, MeshIK is most useful when a basis is carefully chosen to prevent extrapolation artifacts. One drawback to our approach is the loss of secondary kinematic motion, such as the sway of loose cloth. Because MeshIK does not use velocity information, the resulting animation appears damped.
        
      
      
        